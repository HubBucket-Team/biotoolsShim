{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to crawl bio.tools with its API ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows you how to automate the crawling of biotools to filter or transform its content. Please send any comment or feedback to alban.gaignard@univ-nantes.fr. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import json\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "def crawl_biotools(keyword, limit=-1):\n",
    "    \"\"\"\n",
    "    Go through all bio.tools entries and print the tool home page if the keyword is found in the tool description.  \n",
    "    :param limit: an integer value specifying the max number of entries to be crawled, -1 by default, means no limit.\n",
    "    \"\"\"\n",
    "    \n",
    "    http = urllib3.PoolManager()\n",
    "    http.headers['Accept'] = 'application/json'\n",
    "    http.headers['Content-type'] = 'application/json'\n",
    "    \n",
    "    try:\n",
    "        req = http.request('GET', 'https://bio.tools/api/tool/?page=1&?format=json')\n",
    "        count_json = json.loads(req.data.decode('utf-8'))\n",
    "        count = int(count_json['count'])\n",
    "        print(str(count)+ \" available BioTools entries\")\n",
    "\n",
    "        i = 1\n",
    "        nb_tools = 1\n",
    "        has_next_page = True\n",
    "        while has_next_page :\n",
    "            req = http.request('GET', 'https://bio.tools/api/tool/?page=' + str(i) + '&?format=json')\n",
    "            try:\n",
    "                entry = json.loads(req.data.decode('utf-8'))\n",
    "            except JSONDecodeError as e:\n",
    "                print(\"Json decode error for \" + str(req.data.decode('utf-8')))\n",
    "                break\n",
    "            has_next_page = (entry['next'] != None)\n",
    "\n",
    "            for tool in entry['list']:\n",
    "                #print(tool)\n",
    "                if keyword in str(tool['description']).lower():\n",
    "                    print(tool['homepage'])\n",
    "                \n",
    "                nb_tools += 1\n",
    "                progress = nb_tools * 100 / count\n",
    "                if (nb_tools % 500 == 0) :\n",
    "                    print(str(round(progress))+\" % done\")\n",
    "                if ((limit != -1) and (nb_tools >= limit)):\n",
    "                    return\n",
    "            i += 1\n",
    "\n",
    "    except urllib3.exceptions.HTTPError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just crawl the first `1000` entries and search for tools with the `rare disease` keyword in their description field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12157 available BioTools entries\n",
      "https://muccg.github.io/rdrf/\n",
      "http://www.findzebra.com/raredisease\n",
      "4 % done\n",
      "8 % done\n"
     ]
    }
   ],
   "source": [
    "crawl_biotools(\"rare disease\", limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
